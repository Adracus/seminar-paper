\chapter{Cryptography Fundamentals}

\section{Definition}

In present and past it is and always has been important for messages to be
transmitted securely and without the danger of information disclosure.  To meet
these requirements, two main technique have emerged:

One technique is called steganography. This discipline focuses on hiding the
transmission of messages. An example for this can be found in the past: To hide
a message, one used to shave off all hairs of a messenger. Then, the message
was tattooed on the skin. After a while, when the hair grew back, the messenger
was sent to the receiver. Border controls didn't notice this kind of message
transmission at the beginning, but after a while, the controls were informed to
check if a person was transmitting that kind of message. It was genious to
covertly transmit a message, but as soon as the message was discovered, the
information was disclosed. As a result, the message itself should have some
protection, so that only the desired receiver could read it. This was the point
in time when cryptography was born.

Cryptography is the art of encrypting a message so that only receivers with
specific knowledge can obtain the plaintext. The first steps in cryptography
were simple so called substitution ciphers, when some symbols in an alphabet
were switched or rotated.

Imagine the following message:

\vspace{0.5cm}
\textit{attack at dawn}
\vspace{0.5cm}

If one substituted all "a"s with a "z" and all "t"s with an "e", the message
would look like this:

\vspace{0.5cm}
\textit{zeezck ze dzwn}
\vspace{0.5cm}

The message immediately becomes unreadable for someone who doesn't know about
the underlying subsitution. This was, when cryptanalysis comes in: If one looks
at the text, one could for example analyze the frequency of some specific
letters, in this example "z" and "e". The most common letters in the English
alphabet are "e" and "a". One could now try to substitute the letter "z" with
"e". The outcome would be

\vspace{0.5cm}
\textit{aeeack ae dawn}
\vspace{0.5cm}

If one can't guess that the message is \textit{attack at dawn}, one could
continue using cryptanalysis. Since substituting "e" with commonly used letters
fails, one could analyze two letter chains. One of the most common used two
letter combinations with "a" is "at" (as one could have guessed when seeing
"ae").  After substituting "e" with "t", the original message is revealed.

\vspace{0.5cm}
\textit{attack at dawn}
\vspace{0.5cm}

Soon, this procedure also became quite insecure and cryptographers tried to
find new ways to encrypt their messages. It was a gift for them when computers
became commonly available: Procedures requiring masses of mathematical
operations could be executed in less than a second, complex encryption
algorithms appeared.

In modern cryptography there are two main kinds of encrypting and decrypting a
message. The first kind (which was actually also the first one that emerged) is
called \textit{symmetric encryption}. This kind of cryptography uses the same
key for en- and decryption of a message. The previously mentioned substitution
cipher is a \textit{symmetric encryption}, where the substitution table is the
key which is used for en- and decryption.

The other encryption kind is called \textit{asymmetric encryption}. Here, en-
and decryption are seen as different operations. That's why one has to use an
encryption and a decryption key to perform \textit{asymmetric encryption}.

\section{Cryptographic primitives}

The most basic building blocks in the field of cryptography are known as
cryptographic primitives. They include well researched, reliable and accepted
algorithms. Digital signatures, one-way has functions, as and public key
cryptography are examples for cryptographic primitives.

\subsection{Prime numbers}

Prime numbers are natural numbers which are greater than one and only evenly
divisible by one and themselves.

Those numbers play a main role in cryptography as they are commonly used
as a security factor. For example, it is quite difficult to determine the
prime factorization of a number, as none of the existing algorithms solves
the problem in polynomial time. Therefore, prime factorization is a problem
of \textit{nondeterministic polynomial time complexity} and to factorize
large numbers, more time than the age of the universe is needed.

But how can one get prime numbers in a short amount of time? There are two
main known solutions: The first solution is to permute through all positive
natural numbers, more efficient algorithms here are for example the
\textit{Sieve of Erathostenes} or the \textit{Sieve of Atkin}. The other
main approach is the statistical or heuristic approach. Well known methods
here are the \textit{Miller Rabin Test}, a test which delivers the propability
of a number to be a prime or not. This test is used in \textit{OpenSSL} to
find large primes, which makes it possible for users of \textit{OpenSSL} to
not have primes as factors of their RSA modulus.

\subsection{RSA}
	
RSA is an asymmetric encryption standard, developed by Rivest, Shamir and
Adleman. As many other asymmetric encryptions, RSA uses a one way function
with a trapdoor. So in order to understand RSA, one has to understand the
trapdoor function. The RSA trapdoor function relies on Euler's totient function
$\phi$. This function returns the amount of numbers, which are relatively prime
and smaller than a number n. Since prime numbers are only evenly divisible by
1 and themselves, the totient function returns n - 1 for prime numbers. Also,
if you multiply a number with another, the totient function also mutliplies
with the totient function of the other number. An example:

$$\phi(13) = 12$$

$$\phi(11) = 10$$

$$13 * 11 = 143$$

$$\phi(143) = \phi(13) * \phi(11) = 120$$

RSA uses this as follows: To encrypt something, one needs the so called
public key. To decrypt something, one needs the private key. Both keys
contain the so called RSA modulus \textit{n}, which is the product of 
two primes, \textit{p} and \textit{q}. The public key additionally has
the number \textit{e} and the private key has the additional number
\textit{d}. \textit{e} is a number which is coprime to the result of
the totient function of \textit{n}. \textit{d} is the multiplicative inverse
\footnote{The multiplicative inverse of a number a is the number b where $a*b = 1$} of
\textit{e} relative to $\phi(n)$.

Having calculated those needed factors, it is easy to en- and decrypt
a message representative (usually a number). To encrypt a message, one
has to apply the following procedure:

$$c = m^e \Mod{N}$$

Where \textit{c} is the ciphertext and \textit{m} the message representative.

The decryption follows the same procedure, only \textit{c} and \textit{d} are
swapped and the exponent is \textit{d}:

$$m = c^d \Mod{N}$$

Using this procedure without further additions isn't considered safe today:
One plaintext would always produce the same ciphertext. This is why cryptographers
started to use \textit{paddings}. Paddings wrap the plaintext and the padded
text is then encrypted. After decryption, the padding has to be removed in order
to obtain the original plaintext. As a result, to en- and decrypt a message, one
has to know the RSA procedure as well as the used padding.

There are several known so called \textit{padding schemes}. The most widespread
padding was defined in \textit{PKCS \#1}, the first of a family of standards
called Public Key Kryptography Standards. To encrypt, a secret Integer $m$ is 
first encoded into an octet string using the $i2osp$ (integer to octet string 
primitive). This results in an octet string message $M$. $M$ is then
concatenated with a randomly generated octet string as well as some other control
sequences as follows:

$$EM = 0x00 || 0x02 || PS || 0x00 || M$$

Where $EM$ is the encoded message and $PS$ the randomly generated octet string.
$EM$ is then converted into the ciphertext representing integer $C$ with the $os2ip$
operation (octet string to integer primitive).

The decryption then decodes $C$ back into $EM$ and 
uses the leading $0x00$ and $0x02$ to check if the padding was
applied. If so, it uses the second $0x00$ to detect the start of the encrypted
message. In total, this encryption padding enables users to encrypt secrets with
always different integer outcomes.

\section{Cryptographic Protocols}

A cryptographic protocol is a combination of cryptographic primitives,
algorithms and possibly other cryptographic protocols. The protocol describes
how the cryptographic algorithms are used in order to increase confidentiality,
integrity and availablility of the underlying goal.

Cryptographic protocols are usually defined abstract in order to leave language
specific dificulties up to the implementation. Since only the interfaces are
defined, the developers can implement against those and therefore make sure
different implementations are compatible to each other.

\subsection{Diffie-Hellmann}

The Diffie-Hellmann key exchange is a protocol to create a secret key between
two stations. A simple visualization of the Diffie-Hellmann key exchange can
be achieved by using colors. Imagine each of two persons has got a color,
their secret color. To now create a shared secret, they both agree on a
public color. After that, they mix their own private color with that public
color and send the mixture to each other. Both persons are now able to mix
the received mixture with their private color and have then both the same
color, their shared secret.

The previous description is rather informal. To turn those mentioned components
into a procedure which works with computers, some math is required.

First, there has to be a problem, which can't be solved in feasible time
(most commonly, it is an $NP$ hard problem). The most common problem for
the Diffie-Hellmann key exchange is the so called
\textit{Computational-Diffie-Hellmann Problem}:

\vspace{1em}

Given $g$ of a group and the values $A = g^a$ and $B = g^b$, what is
$K = g^{ab}$ with $a$ and $b$ unknown?

\vspace{1em}

The group $g$ is in today's use cases a \textit{cyclic group of prime order}.
In such groups, results of addition and multiplication are repeating
themselves. This is achieved by applying the modulo operation to the result
of e.g. an addition. The modulus has to be a prime number here. An example
for such a group would be

$$n \Mod{13}$$ where $13$ is the prime number and $n$ can be any number.

In practice, the commonly shared secret in the Diffie-Hellman key exchange
is the prime number $p$ determining the cyclic group of prime order and also
the number $g$ which is the base for that group ($g$ has to be a primitive
root of $p$, but explaining this is not relevant for the further explanation).
For the following example, $p = 23$ and $g = 5$.

After having shared the prime number, both participants generate their
private numbers $a$ and $b$, for example $a = 4$ and $b = 10$. Referring to the
previously mentioned visualization, $a$ and $b$ are the private colors of the
two persons wanting to communicate. They both then compute the mixture of their
private color and the ``color'' $g$ with $A = g^a$ and $B = g^b$ which is
$A = 5^4 \Mod{13} = 625 \Mod{13} = 1$ and $B = 5^10 \Mod{13} = 9765625 \Mod{13} = 12$.

The mixtures are then transferred to the communication partner. Each
participant then computes the shared secret $s$ by raising the mixture
to the power of their private color. This results in the following:

$$s = B^a \Mod{p} = 12^4 \Mod{13} = 20736 \Mod{13} = 1$$

or

$$s = A^b \Mod{p} = 1^10 \Mod{13} = 1 \Mod{13} = 1$$

All in all, the Diffie-Hellmann key exchange provides a very secure way
to create a shared secret, but it also is not bulletproof. It is vulnerable
to man-in-the-middle attacks:

Imagine Alice and Bob want to establish a shared secret. They proceed as
described previously by choosing their private colors and the numbers $g$
and $p$. But when asking each other for the mixture, the man in the middle
intercepts as follows: He blocks the request and instead returns a mixture
with his private color and gets the mixtures of Alice and Bob. He then
creates a shared secret with Alice and another shared secret with Bob. With
Alice and Bob thinking they communicate with each other, they actually
communicate with the man in the middle, who is now able to decrypt both
the messages of Alice and Bob. In practice, to prevent such attacks, digital
signatures are applied to the exchanged messages.

\section{Secret Sharing}

The practise of secret sharing takes a secret and splits it into $n$ parts,
called shares. These shares can then be distributed among all participants, the
secret should be shared with.

Most secret sharing schemes are so called $(t, n)$-threshold schemes. Within
those schemes, $t$ is the threshold out of $n$ shares required to be able to
reconstruct the secret, therefore $t \leq n$.

The easiest form of secret sharing is to divide the secret $S$ into shares of
equal length and requiring as many shares as generated to reconstruct the
secret $S$. In a $(t, n)$-threshold scheme that would mean $t = n$.

$$S = "secret"$$
$$s_1 = "se"; s_2 = "cr"; s_3 = "et";$$

Assuming an attacker knows, the secret only consists of alphabetical characters
and that it is six characters long, then there are $6^{26} = 1.7 * 10^{20}$
possibilities.  However, if the attacker is able to obtain a share and knows
where in the secret it belongs, then the possibilities to try decrease to
$4^{26} = 4.5 * 10^{15}$. That's a factor of $37876$.

To avoid this issue a schemes shares below the threshold must not disclose any
information about the secret. This is called \textit{secure} secret sharing.
An example for a secure secret sharing scheme with a $(t, n)$-threshold is the
\textit{XOR} encryption.

$$k = n; k > 0;k, n \in \mathbb{N}$$

$$S \in \mathbb{N}$$

$$j_{i} \in \mathbb{R}; i = n$$

$$\forall m: j_{m} = Random; m = {1, 2, ..., n-1}$$

$$j_{n} = S \oplus j_{1} \oplus j_{2} \oplus ... \oplus j_{n-1} $$

$$S = j_{1} \oplus j_{2} \oplus ... \oplus j_{n-1} \oplus j_{n} $$

To perform this type of scheme $n-1$ random numbers $j_{i}$ must be generated.
Then \textit{XOR} the secret $S$ and the randomly generated numbers. Each of
the generated numbers is a share and the result of the \textit{XOR} chain being
the $n$th share. To reconstruct the secret $S$ simply \textit{XOR} the shares.
This is possible since \textit{XOR} is symmetric.

Although secret sharing through \textit{XOR} encryption is simple and fast, it
becomes impractical once there are many particpants, since all shares are
required to reconstruct the secret and $k = n$ and $n$ is large.

To solve this issue secure $(t, n)$-threshold schemes with $t < n$ must be
used.

\subsection{Shamir's Secret Sharing}

The goals of Shamir's Secret Sharing are that "knowledge of any $k$ or more
$D_i$ pieces makes $D$ easily computable; knowledge of any $k - 1$ or fewer
$D_i$ pieces leaves $D$ completely undetermined (in the sense that all its
possible values are equally likely)." With $D$ being any Data, like a secret.

Shamir's Secret Sharing is more practical since it is a real $(t, n)$-threshold
scheme as $t$ can be smaller than $n$.

$$a_{n}x^n + a_{n-1}x^{n-1} + ... + a_{3}x^3 + a_{2}x^2 + a_{1}x + a_0 $$

Shamir's Secret Sharing uses a polynomial of degree $t$ to generate secrets.
In the generated polynomial in standard form the secret would be $a_0$. The
main idea is that any polynomial can be reconstructed using $degree + 1$ points
on the function. The task of reconstruction can be accomplished using lagrange
interpolation.

In the sense of secret sharing $n$ would be any number of distinct points on
the generated polynomial, except the intercept with the value axis, which is
the secret.

The smallest degree of a polynomial, that is not a single point, is a line, a
polynomial of degree 1, it therefore be reconstructed with two points. It
consequently creates an edge condition of $t \geq 1$.

\subsection{Blakley's scheme}

Blakeley's scheme offers another but referring to the idea quite similar
approach for secret sharing. For a secret which consists of two parts, 
Blakeley encodes the parts into two dimensional, non-parallel linear functions.
The point where the graphs of the two functions intersect represents the
secret. For example, if the secret $s$ is $4$, one has to compute two non-parallel
linear functions intersecting at $y = 4$. It is simple to generate those two
functions if you remember how to compute the point of intersection of two graphs.

For 2D graphs, the intersection can be computed by equating one function with
another function:

$$f(x) = g(x)$$

The standard linear function is $f(x) = mx + b$ where $b$ is the y-intercept and
$m$ is the slope. Two functions are considered parallel if their slope $m$ is 
the same. So now, a more formal definition of two nonparallel lines (which are
always intersecting) is the following:

$$m_1x + b_1 = m_2x + b_2$$

Now, resolve after x:

$$m_1x - m_2x = b_2 - b_1 $$

$$x (m_1 - m_2) = b_2 - b_1$$

$$x = \frac{b_2 - b_1}{m_1 - m_2}$$

Since $b_1$ and $b_2$ as well as $m_1$ and $m_2$ are constants, we can simplify

$$x = \frac{B}{M}$$

To now encode a secret with two parts, one simply has to generate a number $B$
and a number $M \neq 0$ whose quotient is $s$.
$B$ can then be split into $b_1$ and $b_2$ at will
while $M$ has to be split as well, but $m_1 \neq m_2$ (non-parallel lines).

If one instead wants to split a secret for three participants, one has to
create three planes. The point where the three planes intersect
is the secret point (as seeen previous for secrets with two participants).
A line can be described by a slope and an intercept. A plane has one
additional ``slope'' which then spans up the plane. For each further
dimension, an additional component is needed. As a result, the space needed
to describe such planes or more general, hyperplanes, rises with the
threshold of needed participants in a shared secret.

\subsection{Choosing a scheme}

Both secret sharing algorithms achieve security and are able to produce
parts which can be shared across several particpants. Blakeleys scheme
has the important drawback of larger shares the larger the threshold (number
of needed persons to recover a secret) becomes. It can be limited using
several techniques but that would increase the complexity of the encryption.
As a result, a space efficient and for our usecase optimal secret sharing
scheme is \textit{Shamir's Secret Sharing} due to it's simplicity and
security.

\section{Practical Secret Sharing}

The math behind secret sharing schemes has been proven long ago, however, for a
human to use a secret sharing scheme practically some additions have to be
made.

Those include:

\begin{itemize}
  \item cryptographic security
  \item using text instead of numbers
\end{itemize}

\subsection{Finite Fields}

To be cryptographically secure, the operations are to be done within finite
fields. Using shamir's secret sharing scheme that means, that the prime of the
finite field has to be larger than the secret integer. $P > S$

To get such a prime number, one has to test numbers larger than $S$. For example
for the number $S = 25$, the prime number $P$ would be $29$. In that case it is
easy to get a prime number, but for larger numbers it takes a lot of time to get
the next prime number. That's why one needs to have a more efficient method of
getting large prime numbers in a small amount of time.

To always have accurate prime numbers one has to either have a way to efficiently
calculate prime numbers as they are needed or a way to efficiently get a prime
number of a pre-computed set. Each prime number \textit{covers} its on range. For
example, for a secret $S = 14$, the prime number $P = 17$ is suitable as it
\textit{covers} the range from $13$ to $17$. As a result, the range of each
prime number $P$ is from $P_{-1} to P-1$, where $P_{-1}$ is the next smaller
prime number (and for $P = 1$ it is $0$). To represent those ranges one could
invent a range-datastructure (R-Tree is not suitable for this use case), but
if one tried to store the prime numbers from $0$ to $10^{100}$ one would need
an incredibly high amount of storage. As a result, the approach to store all
prime numbers in a large range is quite inefficient in terms of storage and
overall hard to realize.

While it is hard to identify the next prime larger than $S$, there is
a phenomenon that $2^p-1$, is likely to be a prime. These primes are called
Mersenne primes, with $p$ being the $p_{th}$ Mersenne prime. The largest known
Mersenne prime is the 48$_{th}$ and has $17,425,170$ digits.

The only limitation we have is that $P > S$, so any prime larger that the
secret is valid. Using Mersenne primes we can easily use the known primes for
the finite field to use.

This, however, creates a new limitation. A secret can thoretically be larger
than $17,425,170$ digits. To avoid using the secret sharing scheme with larger
secrets. It can previously be devided into several parts making sure, the limit
is never reached for a single part. Then the secret sharing scheme is applied to
each part instead of once per secret.

Fortunately this also has another side effect. If the secret and thus the
calculated shares could be infinitely long, then they would be very inefficient
to persist and process. Splitting allows the data to be processed sequentially,
in a stream. One large secret or shares would require one long loading and
processing operation.

\subsection{Secretly Sharing Strings}

While secure secret sharing schemes work with numbers, secrets generated by
humans are generally strings. Accordingly the schemes must be extended to
support strings.

One solution to the problem is to only allow a specific set of characters. Take
ASCII for example. ASCII has 128 characters. To convert a string to an integer
in a way that is reversible start with a result of 0. Process each character of
the string from left to right one at a time. Multiply the current result with
the length of your set of characters for every character in the string and add
the position of the current character in your set of characters, also known as
a codepoint. (see figure \ref{code:stringToInteger})

\begin{figure}
\begin{lstlisting}
STRING-TO-INTEGER(input)
  result = 0
  while char = next-character(input)
    // 128 is the length of the ASCII Charset
    result = result * 128 + CODEPOINT(char)
  return result
\end{lstlisting}
\caption{Converting an ASCII string to an integer}
\label{code:stringToInteger}
\end{figure}

\textit{DIVMOD} is the inverse function to what was applied to each of the
characters. \textit{DIVMOD} can be seen as a mathematical operation returning
two results. One of the results would be the same as if the modulo operation
was applied and the other the result of integer division.

To reconstruct the string using \textit{DIVMOD} all, that has to be done is
apply \textit{DIVMOD} to the input integer and use the result of the integer
division for the next iteration and the remainder of the modulo operation is
the codepoint of the next character. The string is reconstructed from left to
right. (see figure \ref{code:integerToString})

\begin{figure}
\begin{lstlisting}
INTEGER-TO-STRING(input)
  result = ""
  while input > 0
    // 128 is the length of the ASCII Charset
    input, codepoint = input DIVMOD 128
    result = CHAR-OF-CODEPOINT(codepoint) + result
  return result
\end{lstlisting}
\caption{Converting an integer to an ASCII string}
\label{code:integerToString}
\end{figure}

One of the challenges of secretly sharing strings however, is to support a wide
range of characters. As a result ASCII is not very effective since it limits
the characters immensely, which is a result of ASCII being created to display
only the english language. An encoding, which supports a large set of
characters and is widely used is UTF-8.

Since UTF-8 has a very large set of characters (1114112 to be exact) simply
converting all characters as explained is impractical. To allow the usage of
UTF-8 anyways the easiest solution is to encode the UTF-8 strings as Base64,
then apply the string to integer conversion as explained usind the ASCII
charset. This is possible, since all characters of Base64 are a subset of
ASCII.

\subsection{Share representation}

How shamir's secret sharing can be extended to use finite fields and allowing
UTF-8 have been discussed. To communicate a share, however, a reasonable string
representation has to be chosen.

The main components of a shamir's secret sharing share are the \textit{x} and
the \textit{y} value. Combining them in a single string can be accomplished by
concatenating the two around a dash. While the \textit{x} values will only
become as large as the number of participants, the \textit{y} value can become
any number up to the chosen prime. Since the shares are not human readable,
the \textit{y} value can be compressed by turning them into hex representation
instead of decimal.

Also, to be able to predict the length of a share better, the \textit{x} value
will have a left padding of zeros. If the number of participants is represented
by $n$ then the length of \textit{x} with zero-padding must be
$floor(log_{10}(n))+1$. For example, there are ten participants, then

$$n = 10$$
$$floor(log_{10}(n))+1 = 2$$

Accordingly, the \textit{x} values are represented as $01, 02, 03, 04, 05, 06,
07, 08, 09, 10$.

An example of the explained representation could be \textit{"1-f0"}, which is
equivalent to the point $(1, 240)$.

Additionally the representation is in use by other shamir's secret sharing
libraries, therefore compatibility is more uncomplicated when choosing the
argued representation.

\subsection{Connecting shamir's secret sharing and RSA}

When using shamir's secret sharing, even under consideration of finite fields,
an attacker can estimate the size of a secret since it is somewhat close to the
chosen prime. To eliminate this possibility and introduce more security
features, shamir's secret sharing and RSA are going to be combined. Due to some
limitations of the technologies some adaptions have to be made. These mainly
concern length restrictions of RSA.

When using a RSA public key to encrypt a plain text, the resulting length of
the cipher text is equal to the key size. In case of an 1024 bit RSA key, the
resulting cipher text is always 128 bytes. Within those 128 bytes are a minimum
of 11 bytes PKCS1.5 padding, which leaves up to 117 bytes of actual payload. As
discussed in the previous section the string representation of a share always
contains a dash, leaving 116 bytes. The \textit{x} value is also at least one
character, leaving up to 115 bytes of payload.

Since the chosen prime restricts the size of the shares \textit{y} value, it
has a heavy influence on the total size. To calculate the maximum size of the
payload, the primes have to be examined. The largest possible share can be
represented as only 15 or \textit{f} in terms of hex. Since the mersenne primes
are used, they can be examined for their maximum size. The expected prime is
the mersenne prime below the first prime resulting in more than 117 bytes.

$$11. Mersenne prime: 2^{107}-1$$
$$2^{107} = 800000000000000000000000000_{16}$$

$$12. Mersenne prime: 2^{127}-1$$
$$2^{127} = 80000000000000000000000000000000_{16}$$

$$13. Mersenne prime: 2^{521}-1$$
$$2^{521} = (2*10^{130})_{16}$$

Since the 13$^{th}$ mersenne prime is the first one to exceed 117 digits in
hex, the searched for prime is the 12$^{th}$. Therefore the maximum length for
a single share can be up to 34, since the length of the \textit{y}
representation can be up to 32 bytes. The two additional bytes are for the
dash, and at least one byte for the \textit{x} value representation.

$$floor(log_{10}(80000000000000000000000000000000)) + 1 = 32$$
$$length(x) >= 1$$
$$length(y) = 32$$
$$1 + length(x) + length(y) = total\_length$$
$$1 + length(x) + 32 = total\_length$$
$$1 + 1 + 32 <= total\_length$$
$$34 <= total\_length$$

\chapter{Quality Assurance}

Quality assurance in software covers all aspects of a softwares lifecycle.
From design over the process of development, code revisions, testing and
deployment.

To ensure the best quality software, proven metrics and practises are used.

\section{Code Coverage}

Tests are great but they might be ineffective, test the wrong things
or simply are too few to assure no bugs. This is why there are metrics
for tests or other tools to test whether tests are effective or not.

\subsection{Branch Coverage}

One measure for tests is the so called \textit{coverage}. Coverage
comes in two flavors: The first one is the \textit{Branch Coverage}.
Branch coverage determines whether a test has run through the
required paths of a program. To calculate the branch coverage of a test,
simply divide the number of covered branches $c$ by $b$, where b is the
total number of branches. For example, imagine the following
method:

\begin{figure}
\begin{lstlisting}
abs(int input)
  if (input < 0) return -input
  else return input
\end{lstlisting}
\caption{Absolute method for branch coverage analysis}
\label{code:branchCoverage}
\end{figure}

This method has got two paths: If the number is negative, it will
be multiplied with $-1$ to get the positive absolute of the number.
Otherwise (if it is already positive), it returns the original input.

Now imagine if one wrote the following test \ref{code:oneBranch}

\begin{figure}
\begin{lstlisting}
assert(abs(5), equals(5))
\end{lstlisting}
\caption{One branch covered}
\label{code:oneBranch}
\end{figure}

Of the two paths of the method, only the path with positive numbers is
covered. This leads to $c$ (the number of branches covered) being $1$,
as a result the branch coverage is $\frac{1}{2} = 0.5$. A test with
full branch coverage would be \ref{code:fullBranch}

\begin{figure}
\begin{lstlisting}
assert(abs(5), equals(5))
assert(abs(-5), equals(5))
\end{lstlisting}
\caption{Full branch coverage}
\label{code:fullBranch}
\end{figure}

Full branch coverage is hard to achieve, but important for full testing.

\subsection{Line Coverage}

Another metric for test coverage is the \textit{line coverage}. The line
coverage determines the number of lines covered during a test. It is much
easier to get 100\% line coverage, but tests with 100\% line coverage do
not guarantee the absence of errors.

The programming language in the backend of our service offers good integration
for line coverage analysis while the frontend language for our HTML interface
completely lacked an integration for services offering line coverage analysis.
As a result and as an important measure for quality assurance, an integration
for the coverage analysis service is now established and made public as open
source software. It is now also used by other programmers and companies, such
as for example Google.

It is also important to mention that we are not satisfied with a large line
coverage, we also respect branch coverage (even if it is not calculated by
the existing tools).

\section{Testing}

Tests help to ensure the software meets the requirements and identify bugs.

Testing is an integral part of software development. Today this is especially
the case for test automation. As software becomes more complex it is not
feasible to manually test an application. Most tasks are even to complex for
humans to perform, or they would take a lot of time and be error-prone.

There are several kinds of methods used in automated testing. Some of them
focus on the exhaustiveness of the overall testing, others to ensure that bugs
that already occured do not appear again.

\subsection{Mutation testing}

While testing software is a widely adapted technique, the question is often,
how many tests and more importantly which tests to write. There are numerous
metrics describing the effectiveness of tests.

Mutation tests make sure the existing tests are exhaustive enough. To do so,
the productive code is mutated, hence the name. There are several mutation
techniques.

After each single mutations the tests suite runs and at least one test must
fail. If at least one test fails, the mutation is covered by said test.
Otherwise the mutation is remembered and the next mutation is applied. At the
end all uncovered mutations are listed for the developer to implement. This
method helps developers to test for boundary values, as they are often
automatically discovered by mutations.

Mutations include modification of conditions, literals, return values,
replacing objects with null references, deleting code, and many more, also
depending on the language used.

In the case of integer literals being modified they could be modified by
increasing or decreasing it or setting it to another random value. Then a test
must cover whatever effect the value of the literal has and hopefully
invalidate that cause when it is modified.

When using conditions, it happens easily that a condition is not properly
defined. Therefore mutating rational operators can be useful to determine the
correctness and effectiveness of the defined condition.

Similar to other metrics for tests it is usually very tedious to reach 100\%
mutation coverage, but it ensures that the present tests are as thorough as
possible.

\subsection{Regression testing}

Regression testing are known as tests used to discover bugs in existing
functional code bases, when adding new code, fixing code or modifying
configurations of any kind. These tests often originate from previous bug
reports.

Regression tests avoid, that previously patched bugs do not occur again. In
complex systems adding a new feature often results in breaking corner cases of
other features. When that happens regressions are there to tell the developer
about it.

Obviously these tests should be automated, for the developer to execute when
implementing new features or patches.

\section{Continuous Delivery}

Continuous Delivery embraces test automation, Continuous Integration and the
continuous installation of a software product. It allows teams to perform task
with lots of agility while improving the softwares quality.

A central concept of continuous delivery is known as the deployment pipleline.
The deployment pipeline is the collection of processes a software has to go
through before being released or deployed. An example of a deployment pipeline
could be: 

\begin{itemize}
  \item{passing all tests}
  \item{compiling the source to its targets}
  \item{packaging the application}
  \item{deploying the application to a staging environment}
  \item{executing acceptance tests}
  \item{deploying to a production environment}
\end{itemize}

The trigger for the deployment pipeline is often the used version control
system. Once a new revision is created and uploaded an automated mechanism
triggers the execution of the tests. All tasks trigger the next task once they
finish successfully. If any task fails, preferably the developer who caused the
error should be informed.

This approach allows easy rollback in case a faulty version passed the checks,
since earlier versions of the software can be deployed from version control.

Following this methodology allows teams to ship extremely early and often.
Accompanied by the perfectly fitting use of the so called FeatureToggle. A
FeatureToggle allows a developer to hide a feature which is currently in
development in production, but have it available in the other phases of the
deployment pipeline. It can be thought of as beta testing for each feature.

Integrating the deployment process into daily business of a developer reduces
the risks when deploying to the customer or production.

Continuous delivery does not only improve life for the developer, but also
allows customers to see results in very early stages of the development
process.

\subsection{Continuous Integration}

Continuous integration is the part of Continuous delivery ensuring that a
revisions changes are still allowing the automated tests to pass. The name
originated from the task of integrating all working copies from all developers
of a project. It ensures, that all code being produced still plays well with
the work from the other developers. Today the task of integrating is often done
with the version control system in use.

CI originated from the extreme programming methodology, however, today many
software teams embrace CI without following all extreme programming guidelines.
Typically CI is refered to as a server, service or platform where tests are
executed and the outcome is reported back to the development team. This is also
known as the feedback loop.

The benefit a team applying Continuous integration are numerous. An important
benefit though is that the team eliminate the \textit{it works on my machine}
phenomenon. If the tests pass on the CI server, then the code is classified as
\textit{shippable}. It wipes out the possibility, that a local configuration,
the developer adds is lost along the way, which directly contributes to the
idea of continuous deloyment. The CI becomes the central location of truth
concerning passing tests.

\subsection{Code reviews}

Software teams regularly review their code to improve the softwares quality,
reduce the risk of bugs and provide a consistent coding standard. Code reviews
can take place in different forms, but they all serve the same purpose.

A team can also perform code reviews through version control systems, without
requiring the team to be in the same place. These forms are refered to as
collaborative or crowdsourced reviews.

Code reviews have been researched and they show a significant discovery of
defects in software of 60 percent or more. In terms of continuous deployment it
is critical for a team to embed code reviews in their daily work to make sure
all code is reviewed.

\subsection{Blue green deployment}

A critical feature of continuous deployment is to deploy or release the
software in development up to several times a day. Feature toggles even allows
these systems to be run in production. Troublesome however is when end users
are restricted by downtimes of the application when deploying a new version to
production. To avoid end users ever noticing these downtimes, a method for zero
downtime has to be applied.

Blue green deployments allow deployments to be zero downtime. The approach is
simple. When deploying, there are two identical but seperate environments. The
blue and the green environment. A router, in case of a webserver this can for
example be a reverse proxy or a load balancer, decides to which environment a
requesting user is routed to. Before deployment the service points to the green
environment.

Assuming when the deployment starts, the green environment is live, then the
new version is deployed to the blue environment. In case something fails while
deploying, the router simply does not route to the blue environment. If the
deployment is successful, then the router switches to the blue environment,
thus the new version is now live without any downtime. (see fig
\ref{fig:blue_green_deployment}) Of course, this requires the router to hotload the
new configuration.

\begin{figure}
  \includegraphics[scale=0.55]{pictures/blue_green_deployment.png}
  \caption{Blue green deployment}
  \centering
  \label{fig:blue_green_deployment}
\end{figure}

The two environments, blue and green, are often only used as a metaphor,
because the in the described case updated blue environment is created merely
for the deployment process. Once the deployment has been successful it is
viewed as the green environment and the now outdated environment is removed.
This approach leads to the thought of an immutable infrastructure, where the
philosophy is to never change the state of a deployed system and rather discard
it and fully redeploy it.

\subsection{Rolling updates}

Once applications receive more traffic, then multiple instances have to be
created and load balanced. In such an setting zero downtime deployments are
much easier, since there are already multiple instances of the application
running, there is no single point of failure. Once there is a new version to be
deployed, it is deployed one instance at a time. Since there are multiple
instances running anyways, there will be no noticeable downtime for end users,
thanks to the load balancer, which does not route to an instance that does not
respond yet. (see fig \ref{fig:rolling_updates})

\begin{figure}
  \includegraphics[scale=0.55]{pictures/rolling_updates.png}
  \caption{Rolling updates}
  \centering
  \label{fig:rollgin_updates}
\end{figure}

While there is no established name for this method of zero downtime
deployments, it is sometimes referred to as rolling updates.

\section{Technical dept}
\section{Dependency locking}

\chapter{Duse}

The most important part of a system to securly share secrets is the
cryptographic protocol consisting of multiple cryptographic primitives and
other protocols or algorithms. Since it is a security concerned system,
possible attacks must also be examined.

\section{The Cryptographic Protocol}

The protocol involves a server and a client. During a secrets lifecycle, there
are four events that are relevant for the protocol creating, reading, updating,
and deleting. These are also known as \textit{CRUD}. Most interesting of those
are creating and reading a secret, because deleting just removes all related
data and creating essentially works like creating.

\subsection{Creating a secret}

When creating a secret, the client first has to retrieve all profiles of the
users the secret should be shared with. These profiles contain the public key
of each user.

Now shamir's secret sharing could be used to split the secret into shares for
each user, however, this can lead to problems. Since shamir's secret sharing
requires primes equally large as the input, it can in theory handle any size of
input, but not in practise, as it might take a long time to calculate such a
large prime. Instead the original secret is split into chunks, here called
\textit{secret parts}.

Considering that all the \textit{secret parts} are not too long to compute
primes for each of them, shamir's secret sharing can now be applied.

Once shamir's secret sharing has been applied there are shares for each user
for every secret part. These shares can then be encrypted with the previously
retrieved public keys of each user and then signed with the creating users
private key. All of this data is then send to the server to verify and save.
(see figure \ref{fig:creating_a_secret})

Shamir's secret sharing requires at least two shares to reconstruct a secret,
therefore there will always be a transparent "server"-user which gets a share
that is shared with every participant.

\begin{figure}
  \includegraphics[scale=0.55]{pictures/create_secret_sequence_diagram.png}
  \caption{Creating a secret}
  \label{fig:creating_a_secret}
\end{figure}

\subsection{Retrieving a secret}

Retrieving a secret can be divided into two main parts, server and client
processing.

When a client requests a secret, the servers shares are first decrypted and
then encrypted with the requesting users public key. The shares and additional
metadata, such as the secrets title and a list of participants, is then
returned as the response to the clients request.

Once the client received said response, it decrypts the shares and reconstructs
the secret parts using shamir's secret sharing. Next the secret parts are
concatenated, and the secret is successfully retrieved.

\begin{figure}
  \includegraphics[scale=0.55]{pictures/retrieve_secret_sequence_diagram.png}
  \caption{Retrieving a secret}
  \label{fig:retrieving_a_secret}
\end{figure}

\section{Attacks}

Since this is an atempt to build a system to securely share secrets, possible
attacks must be examined. To prove the security of a protocol the Dolev-Yao
model is often used. The Dolev-Yao model includes abilities one must assume
when evaluating a cryptographic protocol. These include, that the attacker can
listen to all communicated messages, modify them and possibly send them to a
different recipient than the sender intended to. This essentially describes the
classic \textit{Man in the middle} attack.

To validate the system further, the consequences of a completely compromised
system are examined.

\subsection{Man in the middle}

The developed cryptographic protocol has two essential elements, creating and
retrieving a secret. To evaluate the security of these components it is
assumed, that an attacker could modify all data exchanged from client and
server.

\subsubsection{Creating a secret}

There are several requests during the creation of a secret. The attacker cannot
decrypt the encrypted shares, however, the attacker could intercept when the
client requests the user profiles to share the secret with.

The attacker can replace the genuine public keys with its own counterfeit keys.
Then the attacker can decrypt the encrypted shares since the attacker knows the
matching private keys.

The described attack can be weakened when introducing a Public key
infrastructure. Then the clients can verify the authenticity of the received
public key.

There is a second aspect to be examined when creating a secret. The attacker
cannot create modified secrets, since the authenticity through RSA signatures
is checked upon receiving by the server. The attacker is only able to modify a
secret and save it, if the attacker had modified the public key, when the user
was created. To eliminate the possiblity of such an attack the operator of the
system could use a second way of checking the authenticity of a users public
key, an eligibility check for example.

\subsubsection{Retrieving a secret}

Unless an attacker already modified the requesting users public key, at time of
creation of the said user, then the attacker cannot determine the original
secret.

All shares are encrypted with the requesting users public key, thus the shares
are of no use for an attacker, under the condition, that the key establishment
is secure.

\subsection{Compromising the server}

Once the attacker has compromised the server, the abilities match those of the
\textit{Man in the middle} attack, however, with the addition of access to the
database.

Fortunately an attacker cannot decrypt any share, since the users private key
is required to achieve that. Hence any of the stored shares, unless the
attacker previously already tampered the communication of a users public key at
time of user creation is useless for the attacker.

The only assumption an attacker can track is the circumstance, that a secret
has been devided into multiple parts. The number of parts can be used to
approximate the total length. All parts have a maximum length, thus the
attacker can multiply the maximum length by the number of parts and receives an
approximate length of the secret.

$$n: number\,of\,parts$$
$$l: maximum\,length\,of\,secret\,part$$
$$(n - 1) * l \leq length(secret) \leq n * l$$

\chapter{Technology basics}
\section{Web development}
\subsection{Middlewares}
\subsection{RESTful API}
\section{Git}
\subsection{Hooks}

\chapter{Development}
\section{API}
\section{Clients}
\section{Deployment}
\subsection{API}
\subsection{Frontend}
\subsection{Libraries}
